{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caea2551-5c2c-435e-9c0c-806d86bef778",
   "metadata": {},
   "source": [
    "## RagAssistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ba8e9-a004-43f2-9fb5-177ce7c66369",
   "metadata": {},
   "source": [
    "#### Dependendecies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b44bf38b-79ac-4e33-8a55-472a503f2bef",
   "metadata": {},
   "source": [
    "! pip install swarmauri[full]==0.1.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8841cd36-01b0-4cd0-9675-4942b48777f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigman\\anaconda3\\envs\\swarmauri\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from swarmauri.standard.models.concrete.OpenAIModel import OpenAIModel\n",
    "from swarmauri.standard.models.concrete.GroqModel import GroqModel\n",
    "from swarmauri.standard.models.concrete.MistralModel import MistralModel\n",
    "from swarmauri.standard.vector_stores.concrete.TFIDFVectorStore import TFIDFVectorStore\n",
    "from swarmauri.standard.vector_stores.concrete.Doc2VecVectorStore import Doc2VecVectorStore\n",
    "from swarmauri.standard.vector_stores.concrete.MLMVectorStore import MLMVectorStore\n",
    "from swarmauri.standard.conversations.concrete.LimitedSystemContextConversation import LimitedSystemContextConversation\n",
    "from swarmauri.standard.agents.concrete.RagAgent import RagAgent\n",
    "from typing import Any, Optional, Union, Dict\n",
    "from swarmauri.core.messages import IMessage\n",
    "from swarmauri.core.models.IModel import IModel\n",
    "from swarmauri.standard.conversations.base.SystemContextBase import SystemContextBase\n",
    "from swarmauri.standard.agents.base.VectorStoreAgentBase import VectorStoreAgentBase\n",
    "from swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n",
    "from swarmauri.standard.documents.concrete.Document import Document\n",
    "from swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\n",
    "from swarmauri.standard.messages.concrete import (HumanMessage, \n",
    "                                                  SystemMessage,\n",
    "                                                  AgentMessage)\n",
    "from swarmauri.standard.utils.load_documents_from_json import load_documents_from_json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce44f51-9b8f-4a76-96e8-797575bbc443",
   "metadata": {},
   "source": [
    "#### Project Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77bc102-5ef8-4319-aeae-45827fac1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "head=\"\"\"<link rel=\"icon\" href=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAABWFJREFUWEe1V2tsFFUU/s7M7KO77Ra2LRisUArasLjbLqVdaRfQkACCNISgUQJCiCHGfyaYEIMSfIQY/K3GEAUkqNHwUJ6JEoht7ZZuu92uaCktILUJpSy0233PzNU7SClNu9MWvX8ms+ec7/vOuefMvUsYxypxu2dYgHRLS8utcbhjrttdIMJiaG+p69HzJz0Hl8dTKMLYwRiIMeW11ov1hzLFlFZUbSQS93EfNZGaGwz6ujP56wpwOCoeM2abrhOREUBCluXKNv+vbaOBOssXOSVJagRgZgxJOZaaFQr5bj6SAB7sqvS+KoA+J4KJqawucLHWOxpoWaW3joiqOLmqsm3BptqDehXWrcB9gPlub6lkwGcEeAC8xxgjgmDhdgY1xp9E9A7AGmRZfj3U7AvqkWsx43G67+NaWLVeFMVvAIhjxCmKorwcbKr/fry4ExJQ5vFeI9CsTOCMsauBxtri/1yAy+WyCmZbhIgyimaMsd7ueHZPj1/bFr01gQqUG9yerBhAUmZQJrfIcQv8/rQe+YR7oKzS20ZET+tsQSjQWOscD7muAN7p5wOBXAuZp0GQ8g4fPrgjMhCpUWQZ4XAYf3Xf+8Y8XlgIu90OUZJgs9mOv7Jh00dGSehTTOgtLy4eICI2liBtC862tlrzJOtWEK0gYDZAFkbMAgYbANPwaWGMIRpPIJVKI3zrpjZH9vzpMBoNsGaZ+SgO5+LESRAGiFEMYDEGXCWiM32pyJcrSkuj1NDaUSgahJ8JeCpT2RRFwZ2BCG6F7yIWT2iuT0yza88bvWHtackyo8A+BVNtORDFsSZ1iKU9lZKXUdOlK98C9NJo5IqiIhKNItw/gP7IIFT1XiVzrBZMy5uKntbvtIxnuF7EzdthRKL3Gl8QCLk52bDn2pCTbYUoCKPmxsC+oabfO/t5qXlpeZaxRBLRWFwDG4zFwX/nyyBJGmC+PRdmkwn1F85i3ewTmu3otTVYtGQ5Eskk+sL9muC0LGs2LjDbmoUciwXZlixkmU1adbStYrhLvlBH/HLXdXMilYaqqg8pNRoMyM2xYootR8uaB3FBZ06dgDfvDJ51GTT/88E06u+swvKVq4Z8eAJ3ByLoj0SRSj88kYIgwGw0oGROUYIaQx117V1/VimqAk5oNhm1veSE/H14U8XjcXy1fz8W5Aewrpr35oN1pC6JQNiNjZu3wGw2Dxm4YC4gMhhDLJFAIpnS3kVBREnxrHpq/qOrRlXZsUznAgf5LRTCoQMHcPt2X6ZeRX5BATZt3oJ5DsfIiRgZx5iqrtVmpulS5xsA7QWYdroNX52dV3Dyhx/RFmzNSDzS6Corwws1NSgqmj2aEN6t2xc65nw6NLQNbV3TRUFdCyInAWZFUbs+2L3r3e4bNx6u9YRkADNnFiV37t71PhgVCwL/FqAtLUePP+N0aheVjGfB1g1L/YudxgXi6FOkK0VRgV+CqZYvvr6wYCznjAKSDav3GiVhuy5TBgdZVT82VJ58a1ICButWlVlNQjOf5smJYCwWVcqtS0+3TEoAD0o1rD5nkITnJiNAVtg5g+fEskyxupm5K6sXgoSGDNewsfAVQsrT7PP5H0kADy6tqN4jCMKOiVRBYezDYGPtTr0Y3Qr8CyCWVniPCgKt0QPkdsbYsUBj7XoAip7/eAXA4XAYXfMK9lU8KW1kY9wLiTHm75APtF8b3Ob/P65kPJven55fb7MIu0xGmv9gOhhLy2iLJtnuqUtPHtHLerh93BUYCdp1emXJJ8eS5xlAb9ZkLSlcferyRIjv+05aAAco8yz28UM94Kvl/5YmtR5JgKuy+m0wsODFuj2TYv8n6G+wcRzTJW9piwAAAABJRU5ErkJggg==\" type=\"image/png\">\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306fc1bf-a03c-473d-8d1a-2ff839752656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing... this will take a moment.\n",
      "Running on local URL:  http://192.168.1.139:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://192.168.1.139:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot_function error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: Enter yo****** Key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "chatbot_function error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bigman\\anaconda3\\envs\\swarmauri\\lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\bigman\\anaconda3\\envs\\swarmauri\\lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\bigman\\anaconda3\\envs\\swarmauri\\lib\\site-packages\\gradio\\blocks.py\", line 1795, in process_api\n",
      "    data = await self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
      "  File \"C:\\Users\\bigman\\anaconda3\\envs\\swarmauri\\lib\\site-packages\\gradio\\blocks.py\", line 1623, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\bigman\\anaconda3\\envs\\swarmauri\\lib\\site-packages\\gradio\\components\\chatbot.py\", line 217, in postprocess\n",
      "    raise TypeError(\n",
      "TypeError: Expected a list of lists or list of tuples. Received: bg\n"
     ]
    }
   ],
   "source": [
    "class RagAssistant:\n",
    "    def __init__(self, api_key: str = \"\", db_path='prompt_responses.db', \n",
    "                 vector_store_path=None, model_name = \"openai_gpt-4-0125-preview\"):\n",
    "        print('Initializing... this will take a moment.')\n",
    "        self.api_key = api_key\n",
    "        self.db_path = db_path\n",
    "        self.conversation = LimitedSystemContextConversation(max_size=36, system_message_content=\"\")\n",
    "        self.allowed_models =sorted(['groq_llama3-8b-8192',\n",
    "                            'groq_llama3-70b-8192',\n",
    "                            'groq_mixtral-8x7b-32768',\n",
    "                            'groq_gemma-7b-it',\n",
    "                            'mistral_mistral-medium-latest',\n",
    "                            'mistral_mistral-small-latest',\n",
    "                            'mistral_open-mixtral-8x22b',\n",
    "                            'mistral_open-mixtral-8x7b',\n",
    "                            'mistral_open-mistral-7b',\n",
    "                            'mistral_mistral-large-latest',\n",
    "                            'openai_gpt-3.5-turbo',\n",
    "                            'openai_gpt-3.5-turbo-16k',\n",
    "                            'openai_gpt-4-0125-preview'\n",
    "                              ])\n",
    "            \n",
    "        self.model = None\n",
    "        self.retrieval_table = []\n",
    "        self.document_table = []\n",
    "        self.long_term_memory_df = None\n",
    "        self.last_recall_df = None\n",
    "        self.agent = self.initialize_agent()\n",
    "        self.set_model(model_name)\n",
    "        self.css = \"\"\"\n",
    "#chat-dialogue-container {\n",
    "    min-height: 54vh !important;\n",
    "}\n",
    "\n",
    "#document-table-container {\n",
    "    min-height: 80vh !important;\n",
    "}\n",
    "\n",
    "footer {\n",
    "    display: none !important;\n",
    "}\n",
    "\"\"\"\n",
    "        self.setup_gradio_interface()\n",
    "        \n",
    "    def initialize_agent(self):\n",
    "        VS = Doc2VecVectorStore()\n",
    "        agent = RagAgent(name=\"Rag\", model=self.model, conversation=self.conversation, vector_store=VS)\n",
    "        return agent\n",
    "\n",
    "    def set_model(self, provider_model_choice: str):\n",
    "        if provider_model_choice in self.allowed_models:\n",
    "            provider, model_name = provider_model_choice.split('_')\n",
    "            if provider == 'groq':\n",
    "                self.model = GroqModel(api_key=self.api_key, model_name=model_name)\n",
    "                \n",
    "            if provider == 'mistral':\n",
    "                self.model = MistralModel(api_key=self.api_key, model_name=model_name)\n",
    "                \n",
    "            if provider == 'openai':\n",
    "                self.model = OpenAIModel(api_key=self.api_key, model_name=model_name)\n",
    "                \n",
    "            self.agent.model = self.model\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Model name '{model_name}' is not supported. Choose from {self.allowed_models}\")\n",
    "\n",
    "    def change_vectorizer(self, vectorizer: str):\n",
    "        if vectorizer == 'Doc2Vec':\n",
    "            self.agent.vector_store = Doc2VecVectorStore()\n",
    "        if vectorizer == 'MLM':\n",
    "            self.agent.vector_store = MLMVectorStore()\n",
    "        else:\n",
    "            self.agent.vector_store = TFIDFVectorStore()\n",
    "            \n",
    "    \n",
    "    def load_and_filter_json(self, file_info):\n",
    "        # Load JSON file using json library\n",
    "        try:\n",
    "            documents = load_documents_from_json_file(file_info.name)\n",
    "            self.agent.vector_store.documents = []\n",
    "            self.agent.vector_store.add_documents(documents)\n",
    "            self.long_term_memory_df = self.preprocess_documents(documents)\n",
    "            return self.long_term_memory_df\n",
    "        except json.JSONDecodeError:\n",
    "            return \"Invalid JSON file. Please check the file and try again.\"\n",
    "\n",
    "    def preprocess_documents(self, documents):\n",
    "        try:\n",
    "            docs = [d.to_dict() for d in documents]\n",
    "            for d in docs:\n",
    "                metadata = d['metadata']\n",
    "                for each in metadata:\n",
    "                    d[each] = metadata[each]\n",
    "                del d['metadata']\n",
    "                del d['type']\n",
    "                del d['embedding']\n",
    "            df = pd.DataFrame.from_dict(docs)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"postprocess_documents: {e}\")\n",
    "\n",
    "    \n",
    "    def sql_log(self, conversation_id, model_name, prompt, response, start_datetime, end_datetime):\n",
    "        try:\n",
    "            duration = (end_datetime - start_datetime).total_seconds()\n",
    "            start_datetime = start_datetime.isoformat()\n",
    "            end_datetime = end_datetime.isoformat()\n",
    "            conversation_id = str(conversation_id)\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('''CREATE TABLE IF NOT EXISTS conversations\n",
    "                            (id INTEGER PRIMARY KEY AUTOINCREMENT, \n",
    "                            conversation_id TEXT, \n",
    "                            model_name TEXT, \n",
    "                            prompt TEXT, \n",
    "                            response TEXT, \n",
    "                            start_datetime TEXT, \n",
    "                            end_datetime TEXT,\n",
    "                            duration NUMERIC)''')\n",
    "            cursor.execute('''INSERT INTO conversations (\n",
    "                            conversation_id, \n",
    "                            model_name, \n",
    "                            prompt, \n",
    "                            response, \n",
    "                            start_datetime, \n",
    "                            end_datetime,\n",
    "                            duration) VALUES (?, ?, ?, ?, ?, ?, ?)''', \n",
    "                           (conversation_id, \n",
    "                            model_name, \n",
    "                            prompt, \n",
    "                            response, \n",
    "                            start_datetime, \n",
    "                            end_datetime, \n",
    "                            duration))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        except:\n",
    "            raise\n",
    "    \n",
    "    def save_df(self, df):\n",
    "        documents = self.dataframe_to_documents(df)\n",
    "        self.agent.vector_store.documents = []\n",
    "        self.agent.vector_store.add_documents(documents)\n",
    "        self.long_term_memory_df = self.preprocess_documents(documents)\n",
    "        return self.long_term_memory_df\n",
    "\n",
    "    def dataframe_to_documents(self, df):\n",
    "        documents = []\n",
    "        for index, row in df.iterrows():\n",
    "            row_dict = row.to_dict() \n",
    "            id = row_dict.pop(\"id\", \"\")\n",
    "            content = row_dict.pop(\"content\", \"\")\n",
    "            metadata = row_dict  # remaining data becomes metadata\n",
    "        \n",
    "            # Convert the row to dictionary and create a DocumentBase instance\n",
    "            \n",
    "            document = EmbeddedDocument(id=id, content=content, metadata=metadata)\n",
    "            documents.append(document)\n",
    "        return documents\n",
    "    \n",
    "    async def chatbot_function(self, \n",
    "                         message, \n",
    "                         history, \n",
    "                         api_key: str = None, \n",
    "                         model_name: str = None, \n",
    "                         top_k: int = 5, \n",
    "                         temperature: int = 1, \n",
    "                         max_tokens: int = 256):\n",
    "        try:\n",
    "            start_datetime = datetime.now()\n",
    "            if self.agent.vector_store.document_count() == 0:\n",
    "                return \"\", [], [(message, \"⚠️ Add Documents First\")]\n",
    "            else:\n",
    "\n",
    "                \n",
    "                \n",
    "                # Set additional parameters\n",
    "                self.api_key = api_key\n",
    "                self.set_model(model_name)\n",
    "                #print(self.model, self.model.model_name, self.api_key)\n",
    "                \n",
    "                \n",
    "                # Predict\n",
    "                response = self.agent.exec(message, top_k=top_k, model_kwargs={'temperature': temperature, 'max_tokens': max_tokens})\n",
    "                \n",
    "    \n",
    "                # Update Retrieval Document Table\n",
    "                self.last_recall_df = self.preprocess_documents(self.agent.last_similar_documents)\n",
    "                \n",
    "                # Get History\n",
    "                history = [each['content'] for each in self.agent.conversation.to_dict()][1:]\n",
    "                history = [(history[i], history[i+1]) for i in range(0, len(history), 2)]\n",
    "\n",
    "                end_datetime = datetime.now()\n",
    "                self.sql_log(self.agent.conversation.id, model_name, message, response, start_datetime, end_datetime)\n",
    "                return \"\", self.last_recall_df, history\n",
    "        except Exception as e:\n",
    "            gr.Error(f\"{e}\")\n",
    "            print(f\"chatbot_function error: {e}\")\n",
    "            return \"\", [], history\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        # We could clear, but if we create a new instance, we get a new conversation id\n",
    "        self.conversation = LimitedSystemContextConversation(max_size=36, system_message_content=\"\")\n",
    "        self.agent.conversation = self.conversation\n",
    "        return \"\", [], []\n",
    "        \n",
    "    def setup_gradio_interface(self):\n",
    "        with gr.Blocks(css = self.css) as self.retrieval_table:\n",
    "            with gr.Row():\n",
    "                self.retrieval_table = gr.Dataframe(interactive=False, wrap=True, line_breaks=True, elem_id=\"document-table-container\", height=\"800\")\n",
    "\n",
    "        \n",
    "        with gr.Blocks(css = self.css) as self.chat:\n",
    "            with gr.Row():\n",
    "                self.chat_history = gr.Chatbot(label=\"Chat History\", \n",
    "                                           layout=\"panel\", \n",
    "                                           elem_id=\"chat-dialogue-container\", \n",
    "                                           container=True, \n",
    "                                           show_copy_button=True,\n",
    "                                           height=\"70vh\")\n",
    "            with gr.Row():\n",
    "                self.input_box = gr.Textbox(label=\"Type here:\", scale=6)\n",
    "                self.send_button = gr.Button(\"Send\", scale=1)\n",
    "                self.clear_button = gr.Button(\"Clear\", scale=1)\n",
    "                \n",
    "            with gr.Accordion(\"See Details\", open=False):\n",
    "                self.additional_inputs = [\n",
    "                    gr.Textbox(label=\"API Key\", value=self.api_key or \"Enter your API Key\"),\n",
    "                    gr.Dropdown(self.allowed_models, \n",
    "                                value=\"openai_gpt-3.5-turbo\", \n",
    "                                label=\"Model\",\n",
    "                                info=\"Select openai model\"),\n",
    "                    gr.Slider(label=\"Top K\", value=10, minimum=0, maximum=100, step=5, interactive=True),\n",
    "                    gr.Slider(label=\"Temperature\", value=1, minimum=0.0, maximum=1.5, step=0.05, interactive=True),\n",
    "                    gr.Slider(label=\"Max new tokens\", value=256, minimum=256, maximum=4096, step=64, interactive=True)\n",
    "                ]\n",
    "    \n",
    "    \n",
    "            submit_inputs = [self.input_box, self.chat_history]\n",
    "            submit_inputs.extend(self.additional_inputs)\n",
    "            # Function to handle sending messages\n",
    "            self.send_button.click(\n",
    "                self.chatbot_function, \n",
    "                inputs=submit_inputs, \n",
    "                outputs=[self.input_box, self.retrieval_table, self.chat_history]\n",
    "            )\n",
    "        \n",
    "            # Function to handle clearing the chat\n",
    "            self.clear_button.click(\n",
    "                self.clear_chat, \n",
    "                inputs=[], \n",
    "                outputs=[self.input_box, self.retrieval_table, self.chat_history]\n",
    "            )\n",
    "\n",
    "        \n",
    "        with gr.Blocks(css = self.css) as self.document_table:\n",
    "            with gr.Row():\n",
    "                self.file = gr.File(label=\"Upload JSON File\")\n",
    "                self.vectorizer = gr.Dropdown(choices=[\"Doc2Vec\", \"TFIDF\", \"MLM\"], value=\"Doc2Vec\", label=\"Select vectorizer\")\n",
    "                self.load_button = gr.Button(\"load\")\n",
    "            with gr.Row():\n",
    "                self.data_frame = gr.Dataframe(interactive=True, wrap=True, line_breaks=True, elem_id=\"document-table-container\", height=\"700\")\n",
    "            with gr.Row():\n",
    "                self.save_button = gr.Button(\"save\")\n",
    "                \n",
    "            self.vectorizer.change(self.change_vectorizer, inputs=[self.vectorizer], outputs=self.data_frame)\n",
    "            self.load_button.click(self.load_and_filter_json, inputs=[self.file], outputs=self.data_frame)\n",
    "            self.save_button.click(self.save_df, inputs=[self.data_frame])\n",
    "\n",
    "        with gr.Blocks(css = self.css, title=\"Swarmauri Rag Agent\") as self.app:\n",
    "            gr.TabbedInterface(interface_list=[self.chat, self.retrieval_table, self.document_table], \n",
    "                                      tab_names=[\"chat\", \"retrieval\", \"documents\"])\n",
    "\n",
    "    \n",
    "    def launch(self):\n",
    "        self.app.launch(share = False,\n",
    "                        #server_name = \"192.168.1.x\",\n",
    "                        favicon_path = \"./favicon-32x32.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_assistant = RagAssistant(db_path = \"E:\\\\swarmauri_github\\\\prompt_responses.db\")\n",
    "    rag_assistant.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri",
   "language": "python",
   "name": "swarmauri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
